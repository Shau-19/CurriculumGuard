# ğŸ›¡ CurriculumGuard  
**Training-Time Data Control for PyTorch**

[![PyPI](https://img.shields.io/pypi/v/curriculumguard.svg)](https://pypi.org/project/curriculumguard/)  
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

CurriculumGuard is an open-source **training-time data control system** for PyTorch that dynamically adapts **which samples a model sees during training** using live learning dynamics â€” while enforcing stability via rollback-based safety guards.

> Models and optimizers are controlled.  
> Hyperparameters are tuned.  
> **But the data stream itself has been ignored â€” until now.**

---

## ğŸ”¥ Why CurriculumGuard?

Modern datasets are increasingly:
- Noisy  
- Imbalanced  
- Web-scraped  
- Non-stationary  

Yet most training pipelines assume the dataset is **static and trustworthy**.

CurriculumGuard introduces a missing layer in ML infrastructure:

> **Adaptive Data Curriculum with Stability-First Control**

Instead of changing *how* models learn, CurriculumGuard changes **what they learn from â€” safely, during training**.

It works entirely inside your training loop â€” no restarts, no trial explosion.

---

## âš™ Installation

```bash
pip install curriculumguard
```

Verify installation:

```bash
python - <<EOF
from curriculum_guard.core.guard import CurriculumGuard
print(CurriculumGuard)
EOF
```

---

## ğŸš€ Quick Start (v0.2 API)

### 1ï¸âƒ£ Dataset must return sample IDs

CurriculumGuard needs sample-level identity to track learning dynamics.

```python
def __getitem__(self, idx):
    return idx, data, label
```

---

### 2ï¸âƒ£ Minimal usage (Beginner)

```python
from curriculum_guard.curriculum import Curriculum

curriculum = Curriculum.auto(train_dataset)

for ids, x, y in curriculum(train_loader):
    logits = model(x)
    loss   = criterion(logits, y)

    curriculum.step(ids, loss, logits, y)

    loss.mean().backward()
    optimizer.step()
    optimizer.zero_grad()
```

That's it.

* No custom samplers
* No weighting logic
* No curriculum math
* Same PyTorch training loop

---

## ğŸ§  Mental Model

CurriculumGuard acts like an **optimizer for data**:

```
Data â†’ Model â†’ Loss â†’ Curriculum â†’ Safer Data â†’ Model
```

It continuously answers:

> "Which samples are helping learning right now â€” and which are destabilizing it?"

---

## ğŸ§  Signals Observed (Automatically)

| Signal             | What It Represents         |
| ------------------ | -------------------------- |
| EMA loss           | Sample difficulty          |
| Loss variance      | Label noise                |
| Prediction entropy | Shortcut learning          |
| Forgetting events  | Unstable / harmful samples |
| Exposure count     | Over-training risk         |

These signals are **observed, not enforced** â€” safety decisions are made separately.

---

## ğŸ›¡ Safety Model

CurriculumGuard is **conservative by design**.

* Curriculum decisions are **advisory**
* Safety mechanisms are **authoritative**
* Harmful curriculum updates are **rolled back**
* Training stability is never sacrificed

> Policy proposes. Safety decides.

---

## ğŸ“Š Real-World Performance

CurriculumGuard was evaluated across four real-world failure modes: noisy labels, garbage web text, class imbalance, and continual distribution shift.

---

### ğŸ§ª 1ï¸âƒ£ NLP â€” AG News with Garbage Web Text

| Epoch | Baseline Accuracy | CurriculumGuard Accuracy |
|------:|------------------:|-------------------------:|
| 0     | 0.64              | 0.59                     |
| 2     | 0.69              | 0.70                     |
| 5     | â€”                 | **0.72**                 |
| 7     | â€”                 | **0.739**                |

**Observation:** Baseline training plateaus early due to noisy web text. CurriculumGuard keeps improving by suppressing unstable samples.

---

### ğŸ§ª 2ï¸âƒ£ Vision â€” FashionMNIST with 35% Label Noise

| Epoch | Baseline Accuracy | CurriculumGuard Accuracy |
|------:|------------------:|-------------------------:|
| 0     | 0.837             | **0.850**                |
| 2     | 0.840             | **0.859**                |
| 7     | â€”                 | **0.875**                |

**Observation:** Label noise stalls conventional training. CurriculumGuard dynamically downweights corrupted samples.

---

### ğŸ§ª 3ï¸âƒ£ Fraud Detection â€” Credit Card Transactions

| Epoch | Baseline Recall | CurriculumGuard Recall |
|------:|----------------:|-----------------------:|
| 0     | 0.44            | **0.66**               |
| 2     | 0.86            | **0.88**               |
| 5     | â€”               | **0.90**               |

**Observation:** CurriculumGuard rapidly improves minority-class recall without destabilizing training.

---

### ğŸ§ª 4ï¸âƒ£ Continual Learning â€” Distribution Shift

| Phase  | Baseline Accuracy | CurriculumGuard Accuracy      |
|--------|------------------:|------------------------------:|
| Task-A | 0.99              | 0.98                          |
| Task-B | 1.00              | **1.00 (no regression)**      |

**Observation:** Both systems adapt quickly, but CurriculumGuard enforces safety guarantees under distribution drift.

---

## ğŸ§© Progressive API Design (v0.2)

CurriculumGuard scales with user expertise.

### ğŸŸ¢ Beginner (default)

```python
curriculum = Curriculum.auto(dataset)
```

Safe defaults, minimal setup.

---

### ğŸŸ¡ Intermediate (optional tuning)

```python
curriculum = Curriculum.auto(
    dataset,
    sensitivity="medium",   # low | medium | high
    warmup_epochs=2,
    safety=True
)
```

---

### ğŸ”µ Advanced (explicit strategies)

```python
curriculum = Curriculum.custom(
    dataset,
    policy="anti_noise",
    bucketing="quantile",
    safety="rollback",
    entropy_weight=0.3
)
```

---

### ğŸ”´ Research-level (full control)

```python
curriculum = Curriculum.from_components(
    profiler=CustomProfiler(),
    policy=MyPolicy(),
    safety=MySafetyController(),
    bucketer=MyBucketer()
)
```

---

## ğŸ§ª Where CurriculumGuard Shines

* Noisy labels
* Long training runs
* Expensive experiments
* Continual / non-stationary data
* High-risk domains (fraud, medical, finance)

If your dataset is clean, CurriculumGuard stays out of the way.

If it's not â€” it stabilizes learning.

---

## ğŸ“¥ Datasets Used in Benchmarks

The benchmarks above use the following publicly available datasets:

| Dataset | Domain | Source |
|---------|--------|--------|
| **AG News** | NLP | [Kaggle - AG News Classification](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset) |
| **FashionMNIST** | Vision | `sklearn.datasets` (auto-downloads) |
| **Credit Card Fraud** | Fraud Detection | [Kaggle - Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud) |

All datasets are publicly available and free to use for research and benchmarking purposes.

---

## ğŸ“œ License

MIT